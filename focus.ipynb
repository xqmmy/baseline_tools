{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from bert4keras.backend import keras, set_gelu\n",
    "from bert4keras.tokenizers import Tokenizer\n",
    "from bert4keras.models import build_transformer_model\n",
    "from bert4keras.optimizers import Adam, extend_with_piecewise_linear_lr\n",
    "from bert4keras.snippets import sequence_padding, DataGenerator\n",
    "from bert4keras.snippets import open\n",
    "from keras.layers import Lambda, Dense\n",
    "import pandas as pd\n",
    "import random\n",
    "set_gelu('tanh')  # 切换gelu版本\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "maxlen = 128\n",
    "batch_size = 32\n",
    "config_path = '../model/chinese_wwm_ext_L-12_H-768_A-12/bert_config.json'\n",
    "checkpoint_path = '../model/chinese_wwm_ext_L-12_H-768_A-12/bert_model.ckpt'\n",
    "dict_path = '../model/chinese_wwm_ext_L-12_H-768_A-12/vocab.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(valid_rate=0.3):\n",
    "    train_file = \"../data/train.csv\"\n",
    "    test_file = \"../data/test.csv\"\n",
    "    \n",
    "    df_train_data = pd.read_csv(\"../data/train.csv\")\n",
    "    df_test_data = pd.read_csv(\"../data/test.csv\")\n",
    "    \n",
    "    train_data, valid_data, test_data = [], [], []\n",
    "    \n",
    "    for row_i, data in df_train_data.iterrows():\n",
    "        id, level_1, level_2, level_3, level_4, content, label = data\n",
    "        \n",
    "        id, text, label = id, str(level_1) + '\\t' + str(level_2) + '\\t' + \\\n",
    "        str(level_3) + '\\t' + str(level_4) + '\\t' + str(content), label\n",
    "        \n",
    "        if random.random() > valid_rate:\n",
    "            train_data.append( (id, text, int(label)) )\n",
    "        else:\n",
    "            valid_data.append( (id, text, int(label)) )\n",
    "            \n",
    "    for row_i, data in df_test_data.iterrows():\n",
    "        id, level_1, level_2, level_3, level_4, content = data\n",
    "        \n",
    "        id, text, label = id, str(level_1) + '\\t' + str(level_2) + '\\t' + \\\n",
    "        str(level_3) + '\\t' + str(level_4) + '\\t' + str(content), 0\n",
    "        \n",
    "        test_data.append( (id, text, int(label)) )\n",
    "    return train_data, valid_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = load_data(valid_rate=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立分词器\n",
    "tokenizer = Tokenizer(dict_path, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class data_generator(DataGenerator):\n",
    "#     \"\"\"数据生成器\n",
    "#     \"\"\"\n",
    "#     def __iter__(self, random=False):\n",
    "#         batch_token_ids, batch_segment_ids, batch_labels = [], [], []\n",
    "#         for is_end, (text, label) in self.sample(random):\n",
    "#             token_ids, segment_ids = tokenizer.encode(text, maxlen=maxlen)\n",
    "#             batch_token_ids.append(token_ids)\n",
    "#             batch_segment_ids.append(segment_ids)\n",
    "#             batch_labels.append(label)\n",
    "#             if len(batch_token_ids) == self.batch_size or is_end:\n",
    "#                 batch_token_ids = sequence_padding(batch_token_ids)\n",
    "#                 batch_segment_ids = sequence_padding(batch_segment_ids)\n",
    "#                 batch_labels = to_categorical(batch_labels, num_classes)\n",
    "#                 yield [batch_token_ids, batch_segment_ids], batch_labels\n",
    "#                 batch_token_ids, batch_segment_ids, batch_labels = [], [], []\n",
    "class data_generator(DataGenerator):\n",
    "    \"\"\"数据生成器\n",
    "    \"\"\"\n",
    "    def __iter__(self, random=False):\n",
    "        batch_token_ids, batch_segment_ids, batch_labels = [], [], []\n",
    "        for is_end, (id, text, label) in self.sample(random):\n",
    "            token_ids, segment_ids = tokenizer.encode(text, maxlen=maxlen)\n",
    "            batch_token_ids.append(token_ids)\n",
    "            batch_segment_ids.append(segment_ids)\n",
    "            batch_labels.append([label])\n",
    "            if len(batch_token_ids) == self.batch_size or is_end:\n",
    "                batch_token_ids = sequence_padding(batch_token_ids)\n",
    "                batch_segment_ids = sequence_padding(batch_segment_ids)\n",
    "                batch_labels = sequence_padding(batch_labels)\n",
    "                yield [batch_token_ids, batch_segment_ids], batch_labels\n",
    "                batch_token_ids, batch_segment_ids, batch_labels = [], [], []\n",
    "\n",
    "# 转换数据集\n",
    "train_generator = data_generator(train_data, batch_size)\n",
    "valid_generator = data_generator(valid_data, batch_size)\n",
    "test_generator = data_generator(test_data, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input-Token (InputLayer)        (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Input-Segment (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token (Embedding)     (None, None, 768)    16226304    Input-Token[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Segment (Embedding)   (None, None, 768)    1536        Input-Segment[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token-Segment (Add)   (None, None, 768)    0           Embedding-Token[0][0]            \n",
      "                                                                 Embedding-Segment[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Position (PositionEmb (None, None, 768)    393216      Embedding-Token-Segment[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Norm (LayerNormalizat (None, None, 768)    1536        Embedding-Position[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Dropout (Dropout)     (None, None, 768)    0           Embedding-Norm[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-MultiHeadSelfAtte (None, None, 768)    2362368     Embedding-Dropout[0][0]          \n",
      "                                                                 Embedding-Dropout[0][0]          \n",
      "                                                                 Embedding-Dropout[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-MultiHeadSelfAtte (None, None, 768)    0           Transformer-0-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-MultiHeadSelfAtte (None, None, 768)    0           Embedding-Dropout[0][0]          \n",
      "                                                                 Transformer-0-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-0-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-FeedForward (Feed (None, None, 768)    4722432     Transformer-0-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-FeedForward-Dropo (None, None, 768)    0           Transformer-0-FeedForward[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-FeedForward-Add ( (None, None, 768)    0           Transformer-0-MultiHeadSelfAttent\n",
      "                                                                 Transformer-0-FeedForward-Dropout\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-FeedForward-Norm  (None, None, 768)    1536        Transformer-0-FeedForward-Add[0][\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-0-FeedForward-Norm[0]\n",
      "                                                                 Transformer-0-FeedForward-Norm[0]\n",
      "                                                                 Transformer-0-FeedForward-Norm[0]\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Transformer-0-FeedForward-Norm[0]\n",
      "                                                                 Transformer-1-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-1-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-FeedForward (Feed (None, None, 768)    4722432     Transformer-1-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-FeedForward-Dropo (None, None, 768)    0           Transformer-1-FeedForward[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-FeedForward-Add ( (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent\n",
      "                                                                 Transformer-1-FeedForward-Dropout\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-FeedForward-Norm  (None, None, 768)    1536        Transformer-1-FeedForward-Add[0][\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-1-FeedForward-Norm[0]\n",
      "                                                                 Transformer-1-FeedForward-Norm[0]\n",
      "                                                                 Transformer-1-FeedForward-Norm[0]\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-FeedForward-Norm[0]\n",
      "                                                                 Transformer-2-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-2-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-FeedForward (Feed (None, None, 768)    4722432     Transformer-2-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-FeedForward-Dropo (None, None, 768)    0           Transformer-2-FeedForward[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-FeedForward-Add ( (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent\n",
      "                                                                 Transformer-2-FeedForward-Dropout\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-FeedForward-Norm  (None, None, 768)    1536        Transformer-2-FeedForward-Add[0][\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-3-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-2-FeedForward-Norm[0]\n",
      "                                                                 Transformer-2-FeedForward-Norm[0]\n",
      "                                                                 Transformer-2-FeedForward-Norm[0]\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-3-MultiHeadSelfAtte (None, None, 768)    0           Transformer-3-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-3-MultiHeadSelfAtte (None, None, 768)    0           Transformer-2-FeedForward-Norm[0]\n",
      "                                                                 Transformer-3-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-3-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-3-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-3-FeedForward (Feed (None, None, 768)    4722432     Transformer-3-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-3-FeedForward-Dropo (None, None, 768)    0           Transformer-3-FeedForward[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-3-FeedForward-Add ( (None, None, 768)    0           Transformer-3-MultiHeadSelfAttent\n",
      "                                                                 Transformer-3-FeedForward-Dropout\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-3-FeedForward-Norm  (None, None, 768)    1536        Transformer-3-FeedForward-Add[0][\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-4-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-3-FeedForward-Norm[0]\n",
      "                                                                 Transformer-3-FeedForward-Norm[0]\n",
      "                                                                 Transformer-3-FeedForward-Norm[0]\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-4-MultiHeadSelfAtte (None, None, 768)    0           Transformer-4-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-4-MultiHeadSelfAtte (None, None, 768)    0           Transformer-3-FeedForward-Norm[0]\n",
      "                                                                 Transformer-4-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-4-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-4-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-4-FeedForward (Feed (None, None, 768)    4722432     Transformer-4-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-4-FeedForward-Dropo (None, None, 768)    0           Transformer-4-FeedForward[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-4-FeedForward-Add ( (None, None, 768)    0           Transformer-4-MultiHeadSelfAttent\n",
      "                                                                 Transformer-4-FeedForward-Dropout\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-4-FeedForward-Norm  (None, None, 768)    1536        Transformer-4-FeedForward-Add[0][\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-5-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-4-FeedForward-Norm[0]\n",
      "                                                                 Transformer-4-FeedForward-Norm[0]\n",
      "                                                                 Transformer-4-FeedForward-Norm[0]\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-5-MultiHeadSelfAtte (None, None, 768)    0           Transformer-5-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-5-MultiHeadSelfAtte (None, None, 768)    0           Transformer-4-FeedForward-Norm[0]\n",
      "                                                                 Transformer-5-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-5-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-5-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-5-FeedForward (Feed (None, None, 768)    4722432     Transformer-5-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-5-FeedForward-Dropo (None, None, 768)    0           Transformer-5-FeedForward[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-5-FeedForward-Add ( (None, None, 768)    0           Transformer-5-MultiHeadSelfAttent\n",
      "                                                                 Transformer-5-FeedForward-Dropout\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-5-FeedForward-Norm  (None, None, 768)    1536        Transformer-5-FeedForward-Add[0][\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-6-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-5-FeedForward-Norm[0]\n",
      "                                                                 Transformer-5-FeedForward-Norm[0]\n",
      "                                                                 Transformer-5-FeedForward-Norm[0]\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-6-MultiHeadSelfAtte (None, None, 768)    0           Transformer-6-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-6-MultiHeadSelfAtte (None, None, 768)    0           Transformer-5-FeedForward-Norm[0]\n",
      "                                                                 Transformer-6-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-6-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-6-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-6-FeedForward (Feed (None, None, 768)    4722432     Transformer-6-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-6-FeedForward-Dropo (None, None, 768)    0           Transformer-6-FeedForward[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-6-FeedForward-Add ( (None, None, 768)    0           Transformer-6-MultiHeadSelfAttent\n",
      "                                                                 Transformer-6-FeedForward-Dropout\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-6-FeedForward-Norm  (None, None, 768)    1536        Transformer-6-FeedForward-Add[0][\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-7-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-6-FeedForward-Norm[0]\n",
      "                                                                 Transformer-6-FeedForward-Norm[0]\n",
      "                                                                 Transformer-6-FeedForward-Norm[0]\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-7-MultiHeadSelfAtte (None, None, 768)    0           Transformer-7-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-7-MultiHeadSelfAtte (None, None, 768)    0           Transformer-6-FeedForward-Norm[0]\n",
      "                                                                 Transformer-7-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-7-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-7-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-7-FeedForward (Feed (None, None, 768)    4722432     Transformer-7-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-7-FeedForward-Dropo (None, None, 768)    0           Transformer-7-FeedForward[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-7-FeedForward-Add ( (None, None, 768)    0           Transformer-7-MultiHeadSelfAttent\n",
      "                                                                 Transformer-7-FeedForward-Dropout\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-7-FeedForward-Norm  (None, None, 768)    1536        Transformer-7-FeedForward-Add[0][\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-8-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-7-FeedForward-Norm[0]\n",
      "                                                                 Transformer-7-FeedForward-Norm[0]\n",
      "                                                                 Transformer-7-FeedForward-Norm[0]\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-8-MultiHeadSelfAtte (None, None, 768)    0           Transformer-8-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-8-MultiHeadSelfAtte (None, None, 768)    0           Transformer-7-FeedForward-Norm[0]\n",
      "                                                                 Transformer-8-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-8-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-8-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-8-FeedForward (Feed (None, None, 768)    4722432     Transformer-8-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-8-FeedForward-Dropo (None, None, 768)    0           Transformer-8-FeedForward[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-8-FeedForward-Add ( (None, None, 768)    0           Transformer-8-MultiHeadSelfAttent\n",
      "                                                                 Transformer-8-FeedForward-Dropout\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-8-FeedForward-Norm  (None, None, 768)    1536        Transformer-8-FeedForward-Add[0][\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-9-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-8-FeedForward-Norm[0]\n",
      "                                                                 Transformer-8-FeedForward-Norm[0]\n",
      "                                                                 Transformer-8-FeedForward-Norm[0]\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-9-MultiHeadSelfAtte (None, None, 768)    0           Transformer-9-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-9-MultiHeadSelfAtte (None, None, 768)    0           Transformer-8-FeedForward-Norm[0]\n",
      "                                                                 Transformer-9-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-9-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-9-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-9-FeedForward (Feed (None, None, 768)    4722432     Transformer-9-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-9-FeedForward-Dropo (None, None, 768)    0           Transformer-9-FeedForward[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-9-FeedForward-Add ( (None, None, 768)    0           Transformer-9-MultiHeadSelfAttent\n",
      "                                                                 Transformer-9-FeedForward-Dropout\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-9-FeedForward-Norm  (None, None, 768)    1536        Transformer-9-FeedForward-Add[0][\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-10-MultiHeadSelfAtt (None, None, 768)    2362368     Transformer-9-FeedForward-Norm[0]\n",
      "                                                                 Transformer-9-FeedForward-Norm[0]\n",
      "                                                                 Transformer-9-FeedForward-Norm[0]\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-10-MultiHeadSelfAtt (None, None, 768)    0           Transformer-10-MultiHeadSelfAtten\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-10-MultiHeadSelfAtt (None, None, 768)    0           Transformer-9-FeedForward-Norm[0]\n",
      "                                                                 Transformer-10-MultiHeadSelfAtten\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-10-MultiHeadSelfAtt (None, None, 768)    1536        Transformer-10-MultiHeadSelfAtten\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-10-FeedForward (Fee (None, None, 768)    4722432     Transformer-10-MultiHeadSelfAtten\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-10-FeedForward-Drop (None, None, 768)    0           Transformer-10-FeedForward[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-10-FeedForward-Add  (None, None, 768)    0           Transformer-10-MultiHeadSelfAtten\n",
      "                                                                 Transformer-10-FeedForward-Dropou\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-10-FeedForward-Norm (None, None, 768)    1536        Transformer-10-FeedForward-Add[0]\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-11-MultiHeadSelfAtt (None, None, 768)    2362368     Transformer-10-FeedForward-Norm[0\n",
      "                                                                 Transformer-10-FeedForward-Norm[0\n",
      "                                                                 Transformer-10-FeedForward-Norm[0\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-11-MultiHeadSelfAtt (None, None, 768)    0           Transformer-11-MultiHeadSelfAtten\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-11-MultiHeadSelfAtt (None, None, 768)    0           Transformer-10-FeedForward-Norm[0\n",
      "                                                                 Transformer-11-MultiHeadSelfAtten\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-11-MultiHeadSelfAtt (None, None, 768)    1536        Transformer-11-MultiHeadSelfAtten\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-11-FeedForward (Fee (None, None, 768)    4722432     Transformer-11-MultiHeadSelfAtten\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-11-FeedForward-Drop (None, None, 768)    0           Transformer-11-FeedForward[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-11-FeedForward-Add  (None, None, 768)    0           Transformer-11-MultiHeadSelfAtten\n",
      "                                                                 Transformer-11-FeedForward-Dropou\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-11-FeedForward-Norm (None, None, 768)    1536        Transformer-11-FeedForward-Add[0]\n",
      "__________________________________________________________________________________________________\n",
      "CLS-token (Lambda)              (None, 768)          0           Transformer-11-FeedForward-Norm[0\n",
      "__________________________________________________________________________________________________\n",
      "dense_79 (Dense)                (None, 2)            1538        CLS-token[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 101,678,594\n",
      "Trainable params: 101,678,594\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 加载预训练模型\n",
    "bert = build_transformer_model(\n",
    "    config_path=config_path,\n",
    "    checkpoint_path=checkpoint_path,\n",
    "    model='roberta',\n",
    "    return_keras_model=False,\n",
    ")\n",
    "output = Lambda(lambda x: x[:, 0], name='CLS-token')(bert.model.output)\n",
    "output = Dense(\n",
    "    units=num_classes,\n",
    "    activation='softmax',\n",
    "    kernel_initializer=bert.initializer\n",
    ")(output)\n",
    "\n",
    "model = keras.models.Model(bert.model.input, output)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 派生为带分段线性学习率的优化器。\n",
    "# 其中name参数可选，但最好填入，以区分不同的派生优化器。\n",
    "AdamLR = extend_with_piecewise_linear_lr(Adam, name='AdamLR')\n",
    "\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    # optimizer=Adam(1e-5),  # 用足够小的学习率\n",
    "    optimizer=AdamLR(learning_rate=1e-4, lr_schedule={\n",
    "        1000: 1,\n",
    "        2000: 0.1\n",
    "    }),\n",
    "    metrics=['accuracy'],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def evaluate(data):\n",
    "#     total, right = 0., 0.\n",
    "#     for x_true, y_true in data:\n",
    "#         y_pred = model.predict(x_true).argmax(axis=1)\n",
    "#         y_true = y_true.argmax(axis=1)\n",
    "#         total += len(y_true)\n",
    "#         right += (y_true == y_pred).sum()\n",
    "#     return right / total\n",
    "\n",
    "def evaluate(data):\n",
    "    total, right = 0., 0.\n",
    "    for x_true, y_true in data:\n",
    "        y_pred = model.predict(x_true).argmax(axis=1)\n",
    "        y_true = y_true[:, 0]\n",
    "        total += len(y_true)\n",
    "        right += (y_true == y_pred).sum()\n",
    "    return right / total\n",
    "\n",
    "\n",
    "\n",
    "class Evaluator(keras.callbacks.Callback):\n",
    "    \"\"\"评估与保存\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.best_val_acc = 0.\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_acc = evaluate(valid_generator)\n",
    "        if val_acc > self.best_val_acc:\n",
    "            self.best_val_acc = val_acc\n",
    "            model.save_weights('best_model.weights')\n",
    "#         test_acc = evaluate(test_generator)\n",
    "#         print(\n",
    "#             u'val_acc: %.5f, best_val_acc: %.5f, test_acc: %.5f\\n' %\n",
    "#             (val_acc, self.best_val_acc, test_acc)\n",
    "#         )\n",
    "        test_acc = evaluate(valid_generator)\n",
    "        print(\n",
    "            u'val_acc: %.5f, best_val_acc: %.5f, test_acc: %.5f\\n' %\n",
    "            (val_acc, self.best_val_acc, test_acc)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "263"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "263/263 [==============================] - 138s 523ms/step - loss: 0.1912 - accuracy: 0.9139\n",
      "val_acc: 0.97798, best_val_acc: 0.97798, test_acc: 0.97798\n",
      "\n",
      "Epoch 2/10\n",
      "263/263 [==============================] - 127s 482ms/step - loss: 0.0754 - accuracy: 0.9743\n",
      "val_acc: 0.97547, best_val_acc: 0.97798, test_acc: 0.97547\n",
      "\n",
      "Epoch 3/10\n",
      "263/263 [==============================] - 126s 480ms/step - loss: 0.0585 - accuracy: 0.9825\n",
      "val_acc: 0.97770, best_val_acc: 0.97798, test_acc: 0.97770\n",
      "\n",
      "Epoch 4/10\n",
      "263/263 [==============================] - 127s 482ms/step - loss: 0.0805 - accuracy: 0.9735\n",
      "val_acc: 0.92224, best_val_acc: 0.97798, test_acc: 0.92224\n",
      "\n",
      "Epoch 5/10\n",
      "263/263 [==============================] - 127s 481ms/step - loss: 0.0915 - accuracy: 0.9697\n",
      "val_acc: 0.97742, best_val_acc: 0.97798, test_acc: 0.97742\n",
      "\n",
      "Epoch 6/10\n",
      "263/263 [==============================] - 126s 480ms/step - loss: 0.0402 - accuracy: 0.9879\n",
      "val_acc: 0.97770, best_val_acc: 0.97798, test_acc: 0.97770\n",
      "\n",
      "Epoch 7/10\n",
      "263/263 [==============================] - 126s 481ms/step - loss: 0.0188 - accuracy: 0.9952\n",
      "val_acc: 0.97938, best_val_acc: 0.97938, test_acc: 0.97938\n",
      "\n",
      "Epoch 8/10\n",
      "263/263 [==============================] - 126s 481ms/step - loss: 0.0120 - accuracy: 0.9973\n",
      "val_acc: 0.97993, best_val_acc: 0.97993, test_acc: 0.97993\n",
      "\n",
      "Epoch 9/10\n",
      "263/263 [==============================] - 127s 481ms/step - loss: 0.0092 - accuracy: 0.9977\n",
      "val_acc: 0.97882, best_val_acc: 0.97993, test_acc: 0.97882\n",
      "\n",
      "Epoch 10/10\n",
      "263/263 [==============================] - 127s 482ms/step - loss: 0.0073 - accuracy: 0.9981\n",
      "val_acc: 0.97910, best_val_acc: 0.97993, test_acc: 0.97910\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fe64c2ccf40>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "        train_generator.forfit(),\n",
    "        steps_per_epoch=len(train_generator),\n",
    "        epochs=10,\n",
    "        callbacks=[evaluator]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final test acc: 0.979933\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('best_model.weights')\n",
    "# print(u'final test acc: %05f\\n' % (evaluate(test_generator)))\n",
    "print(u'final test acc: %05f\\n' % (evaluate(valid_generator)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final test acc: 0.997741\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(u'final test acc: %05f\\n' % (evaluate(train_generator)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_pred3(test_data):\n",
    "    id_ids, y_pred_ids = [], []\n",
    "    for id, text, label in test_data:\n",
    "        token_ids, segment_ids = tokenizer.encode(text, maxlen=maxlen)\n",
    "        token_ids = sequence_padding([token_ids])\n",
    "        segment_ids = sequence_padding([segment_ids])\n",
    "        y_pred = int(model.predict([token_ids, segment_ids]).argmax(axis=1)[0])\n",
    "        id_ids.append(id)\n",
    "        y_pred_ids.append(y_pred)\n",
    "    return id_ids, y_pred_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_ids, y_pred_ids = data_pred3(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_save = pd.DataFrame()\n",
    "df_save['id'] = id_ids\n",
    "df_save['label'] = y_pred_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_save.to_csv('result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bs",
   "language": "python",
   "name": "bs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
